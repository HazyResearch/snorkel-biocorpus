"""

Transform PubMed text generated by extract_pubmed.py into tokenized
standoff format. This leverages 2 external software tools that fix
tokenization errors when using CoreNLP and spaCy are
used on biomedical text, primarily:

 1) Errors tokenizing chemical names
 2) Correctly identifying sentence boundaries in the presence of complex chemical entities.

I. Sentence Boundary Detection (SBD)

    GENIA Sentence Splitter v1.0.0
    http://www.nactem.ac.uk/y-matsu/geniass/
    They report F1=99.7 test set performance

II. Word Tokenization

    ChemTok v1.0.1
    https://sourceforge.net/projects/oscar3-chem/files/chemtok/

    From OSCAR (Open Source Chemistry Analysis Routines) toolkit:
    https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3117806/
    "ChemicalTagger: A tool for semantic text-mining in chemistry"

    NOTE: This version has been subsumed by OSCAR4. It's unclear if
    the latest version provides improved tokenization.

This script assumes these binaries are installed in bin/

"""
import os
import glob
import argparse
from subprocess import check_output


def main(args):

    filelist = glob.glob("{}/*".format(args.inputdir)) if os.path.isdir(args.inputdir) else [args.inputdir]
    filelist = [fp for fp in filelist if not os.path.isdir(fp)]

    #
    # I: Sentence Boundary Detection
    #
    outpath = "{}/sentences/".format(args.outputdir)
    if not os.path.exists(outpath):
        os.mkdir(outpath)

    for fp in filelist:
        cwd = os.getcwd()
        os.chdir("bin/geniass")
        op = ".".join(fp.split(".")[0:-1] + ["sentences"] + fp.split(".")[-1:])
        op = "{}/sentences/{}".format(args.outputdir, op.split("/")[-1])

        cmd = "./geniass {} {}".format(fp, op)
        out = check_output(cmd.split())
        os.chdir(cwd)
        print "SBD", fp, "DONE"

    #
    # II. Tokenization
    #
    filelist = glob.glob("{}/sentences/*.sentences.txt".format(args.outputdir)) if os.path.isdir(args.outputdir) \
        else [args.outputdir]
    filelist = [fp for fp in filelist if not os.path.isdir(fp)]

    outpath = "{}/tokens/".format(args.outputdir)
    if not os.path.exists(outpath):
        os.mkdir(outpath)

    for fp in filelist:
        cwd = os.getcwd()
        os.chdir("bin/chemtok-1.0.1")
        op = ".".join(fp.split(".")[0:-1] + ["tokens"] + fp.split(".")[-1:])
        op = "{}/tokens/{}".format(args.outputdir, op.split("/")[-1])
        cmd = "java -jar chemtok-1.0.1.jar < {} > {}".format(fp, op)
        os.system(cmd)
        os.chdir(cwd)
        print "Tokenization", fp, "DONE"

if __name__ == '__main__':

    argparser = argparse.ArgumentParser()
    argparser.add_argument("-i", "--inputdir", type=str, default=None,  help="input directory or file")
    argparser.add_argument("-o", "--outputdir", type=str, default=".", help="outout directory")
    args = argparser.parse_args()
    main(args)



